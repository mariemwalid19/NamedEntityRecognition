{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccf9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import spacy\n",
    "import spacy.cli\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import EntityRuler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from spacy import displacy\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import csv\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c95021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting our Named Entity Recognition Journey!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting our Named Entity Recognition Journey!\")\n",
    "\n",
    "CONFIG = {\n",
    "    'data_paths': {\n",
    "        'train': 'Dataset/train.txt',\n",
    "        'valid': 'Dataset/valid.txt', \n",
    "        'test': 'Dataset/test.txt'\n",
    "    },\n",
    "    'models_to_compare': ['en_core_web_sm', 'en_core_web_md'],\n",
    "    'output_dir': 'ner_results'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096686f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing our data loader...\")\n",
    "        \n",
    "    def load_conll_file(self, filepath: str) -> List[Dict]:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Oops! Can't find the file: {filepath}\")\n",
    "            \n",
    "        sentences = []\n",
    "        current_tokens = []\n",
    "        current_tags = []\n",
    "        \n",
    "        print(f\"Reading {filepath}...\")\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line:\n",
    "                    if current_tokens:\n",
    "                        sentences.append({\n",
    "                            'tokens': current_tokens.copy(),\n",
    "                            'ner_tags': current_tags.copy(),\n",
    "                            'sentence_id': len(sentences)\n",
    "                        })\n",
    "                        current_tokens.clear()\n",
    "                        current_tags.clear()\n",
    "                    continue\n",
    "                \n",
    "                if line.startswith('-DOCSTART-'):\n",
    "                    continue\n",
    "                    \n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    token = parts[0]\n",
    "                    ner_tag = parts[-1] \n",
    "                    \n",
    "                    current_tokens.append(token)\n",
    "                    current_tags.append(ner_tag)\n",
    "        \n",
    "        if current_tokens:\n",
    "            sentences.append({\n",
    "                'tokens': current_tokens.copy(),\n",
    "                'ner_tags': current_tags.copy(),\n",
    "                'sentence_id': len(sentences)\n",
    "            })\n",
    "        \n",
    "        print(f\"Successfully loaded {len(sentences)} sentences from {filepath}\")\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ef038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataProcessor:\n",
    "    \n",
    "    def __init__(self, spacy_model='en_core_web_sm'):\n",
    "        print(\"Setting up data processor...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(spacy_model)\n",
    "            print(f\"Loaded spaCy model: {spacy_model}\")\n",
    "        except IOError:\n",
    "            print(f\"Model {spacy_model} not found. Let me download it for you...\")\n",
    "            spacy.cli.download(spacy_model)  # Fixed: use spacy.cli directly\n",
    "            self.nlp = spacy.load(spacy_model)\n",
    "    \n",
    "    def convert_iob_to_spans(self, tokens: List[str], tags: List[str]) -> Tuple[str, List[Tuple]]:\n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        char_positions = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        for token in tokens:\n",
    "            start = current_pos\n",
    "            end = start + len(token)\n",
    "            char_positions.append((start, end))\n",
    "            current_pos = end + 1  \n",
    "        \n",
    "        spans = []\n",
    "        current_entity = None\n",
    "        current_start = None\n",
    "        current_label = None\n",
    "        \n",
    "        for i, tag in enumerate(tags):\n",
    "            if tag == 'O': \n",
    "                if current_entity is not None:\n",
    "                    end_pos = char_positions[i-1][1]\n",
    "                    spans.append((current_start, end_pos, current_label))\n",
    "                    current_entity = None\n",
    "            else:\n",
    "                tag_parts = tag.split('-', 1)\n",
    "                prefix = tag_parts[0]\n",
    "                label = tag_parts[1] if len(tag_parts) > 1 else 'MISC'\n",
    "                \n",
    "                if prefix == 'B': \n",
    "                    if current_entity is not None:\n",
    "                        end_pos = char_positions[i-1][1]\n",
    "                        spans.append((current_start, end_pos, current_label))\n",
    "                    \n",
    "                    current_entity = True\n",
    "                    current_start = char_positions[i][0]\n",
    "                    current_label = label\n",
    "                    \n",
    "                elif prefix == 'I': \n",
    "                    if current_entity is None:\n",
    "                        current_entity = True\n",
    "                        current_start = char_positions[i][0]\n",
    "                        current_label = label\n",
    "        \n",
    "        if current_entity is not None:\n",
    "            end_pos = char_positions[-1][1]\n",
    "            spans.append((current_start, end_pos, current_label))\n",
    "        \n",
    "        return text, spans\n",
    "    \n",
    "    def process_sentences(self, sentences: List[Dict]) -> List[Dict]:\n",
    "        print(f\"Processing {len(sentences)} sentences...\")\n",
    "        \n",
    "        processed = []\n",
    "        for sentence in sentences:\n",
    "            text, spans = self.convert_iob_to_spans(\n",
    "                sentence['tokens'], \n",
    "                sentence['ner_tags']\n",
    "            )\n",
    "            \n",
    "            processed.append({\n",
    "                'text': text,\n",
    "                'entities': spans,\n",
    "                'original_tokens': sentence['tokens'],\n",
    "                'original_tags': sentence['ner_tags'],\n",
    "                'sentence_id': sentence['sentence_id']\n",
    "            })\n",
    "        \n",
    "        print(\"Data processing complete!\")\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fe7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedNER:\n",
    "    \n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.patterns = []\n",
    "        print(\"Rule-based NER system initialized!\")\n",
    "    \n",
    "    def learn_patterns_from_data(self, training_data: List[Dict], min_frequency=2):\n",
    "        print(\"Learning patterns from training data...\")\n",
    "        \n",
    "        entity_counter = Counter()\n",
    "        \n",
    "        for example in training_data:\n",
    "            text = example['text']\n",
    "            for start, end, label in example['entities']:\n",
    "                entity_text = text[start:end].strip()\n",
    "                if entity_text:\n",
    "                    entity_counter[(entity_text, label)] += 1\n",
    "        \n",
    "        patterns = []\n",
    "        for (entity, label), count in entity_counter.most_common():\n",
    "            if count >= min_frequency:\n",
    "                patterns.append({\n",
    "                    'label': label,\n",
    "                    'pattern': entity,\n",
    "                    'frequency': count\n",
    "                })\n",
    "        \n",
    "        self.patterns = patterns[:3000]  \n",
    "        print(f\"Learned {len(self.patterns)} patterns from training data\")\n",
    "        \n",
    "        if 'entity_ruler' in self.nlp.pipe_names:\n",
    "            self.nlp.remove_pipe('entity_ruler')\n",
    "        \n",
    "        ruler = self.nlp.add_pipe('entity_ruler', before='ner')\n",
    "        ruler.add_patterns(self.patterns)\n",
    "        \n",
    "        return self.patterns\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> List[Dict]:\n",
    "        predictions = []\n",
    "        \n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "            predictions.append({\n",
    "                'text': text,\n",
    "                'entities': entities\n",
    "            })\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4d8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedNER:\n",
    "    \n",
    "    def __init__(self, base_model='en_core_web_sm'):\n",
    "        print(\"Initializing model-based NER system...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(base_model)\n",
    "        except IOError:\n",
    "            print(f\"ðŸ“¥ Downloading {base_model}...\")\n",
    "            spacy.cli.download(base_model)\n",
    "            self.nlp = spacy.load(base_model)\n",
    "            \n",
    "        print(f\"Model-based NER ready with {base_model}\")\n",
    "    \n",
    "    def prepare_training_data(self, processed_data: List[Dict]) -> List[Tuple]:\n",
    "        training_data = []\n",
    "        \n",
    "        for example in processed_data:\n",
    "            entities_dict = {\"entities\": example['entities']}\n",
    "            training_data.append((example['text'], entities_dict))\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def train_model(self, training_data: List[Tuple], validation_data: List[Tuple] = None, iterations=10, early_stopping=True):\n",
    "        print(f\"Training model for {iterations} iterations...\")\n",
    "        \n",
    "        ner = self.nlp.get_pipe('ner')\n",
    "        for text, annotations in training_data:\n",
    "            for start, end, label in annotations['entities']:\n",
    "                ner.add_label(label)\n",
    "        \n",
    "        examples = []\n",
    "        for text, annotations in training_data:\n",
    "            doc = self.nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            examples.append(example)\n",
    "        \n",
    "        optimizer = self.nlp.resume_training()\n",
    "        \n",
    "        best_score = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            print(f\"Training iteration {iteration + 1}/{iterations}\")\n",
    "            \n",
    "            random.shuffle(examples)\n",
    "            losses = {}\n",
    "            \n",
    "            batches = minibatch(examples, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                self.nlp.update(batch, drop=0.2, losses=losses, sgd=optimizer)\n",
    "            \n",
    "            print(f\"   Loss: {losses.get('ner', 0):.4f}\")\n",
    "            \n",
    "            if early_stopping and validation_data:\n",
    "                val_score = self._evaluate_on_validation(validation_data)\n",
    "                print(f\"   Validation F1: {val_score:.4f}\")\n",
    "                \n",
    "                if val_score > best_score:\n",
    "                    best_score = val_score\n",
    "                    patience_counter = 0\n",
    "                    self._save_best_model()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= 3:\n",
    "                        print(\"Early stopping triggered!\")\n",
    "                        self._load_best_model()\n",
    "                        break\n",
    "        \n",
    "        print(\"Model training complete!\")\n",
    "        return self.nlp\n",
    "    \n",
    "    def _evaluate_on_validation(self, validation_data):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text, annotations in validation_data[:100]: \n",
    "            doc = self.nlp(text)\n",
    "            predicted_entities = set((ent.start_char, ent.end_char, ent.label_) for ent in doc.ents)\n",
    "            true_entities = set(annotations['entities'])\n",
    "            \n",
    "            correct += len(predicted_entities & true_entities)\n",
    "            total += len(true_entities) + len(predicted_entities - true_entities)\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n",
    "    \n",
    "    def _save_best_model(self):\n",
    "        self._best_model_bytes = self.nlp.to_bytes()\n",
    "    \n",
    "    def _load_best_model(self):\n",
    "        if hasattr(self, '_best_model_bytes'):\n",
    "            self.nlp.from_bytes(self._best_model_bytes)\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> List[Dict]:\n",
    "        predictions = []\n",
    "        \n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "            predictions.append({\n",
    "                'text': text,\n",
    "                'entities': entities\n",
    "            })\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd8b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEREvaluator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"NER Evaluator initialized!\")\n",
    "        \n",
    "        self.label_mapping = {\n",
    "            'PERSON': 'PER', 'PER': 'PER',\n",
    "            'ORG': 'ORG', 'ORGANIZATION': 'ORG',\n",
    "            'GPE': 'LOC', 'LOC': 'LOC', 'LOCATION': 'LOC', 'FAC': 'LOC',\n",
    "            'NORP': 'MISC', 'PRODUCT': 'MISC', 'EVENT': 'MISC',\n",
    "            'WORK_OF_ART': 'MISC', 'LAW': 'MISC', 'LANGUAGE': 'MISC',\n",
    "            'DATE': 'MISC', 'TIME': 'MISC', 'PERCENT': 'MISC',\n",
    "            'MONEY': 'MISC', 'QUANTITY': 'MISC', 'ORDINAL': 'MISC',\n",
    "            'CARDINAL': 'MISC'\n",
    "        }\n",
    "    \n",
    "    def normalize_predictions(self, predictions: List[Dict]) -> List[Dict]:\n",
    "        normalized = []\n",
    "        \n",
    "        for pred in predictions:\n",
    "            normalized_entities = []\n",
    "            for start, end, label in pred['entities']:\n",
    "                normalized_label = self.label_mapping.get(label, 'MISC')\n",
    "                normalized_entities.append((start, end, normalized_label))\n",
    "            \n",
    "            normalized.append({\n",
    "                'text': pred['text'],\n",
    "                'entities': normalized_entities\n",
    "            })\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def calculate_metrics(self, true_data: List[Dict], predicted_data: List[Dict]) -> Dict:\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        \n",
    "        for true_example, pred_example in zip(true_data, predicted_data):\n",
    "            true_entities = set(true_example['entities'])\n",
    "            pred_entities = set(pred_example['entities'])\n",
    "            \n",
    "            true_positive += len(true_entities & pred_entities)\n",
    "            false_positive += len(pred_entities - true_entities)\n",
    "            false_negative += len(true_entities - pred_entities)\n",
    "        \n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positive': true_positive,\n",
    "            'false_positive': false_positive,\n",
    "            'false_negative': false_negative\n",
    "        }\n",
    "    \n",
    "    def detailed_analysis(self, true_data: List[Dict], predicted_data: List[Dict]) -> Dict:\n",
    "        all_labels = set()\n",
    "        for example in true_data:\n",
    "            for _, _, label in example['entities']:\n",
    "                all_labels.add(label)\n",
    "        \n",
    "        per_label_metrics = {}\n",
    "        \n",
    "        for label in all_labels:\n",
    "            label_true = []\n",
    "            label_pred = []\n",
    "            \n",
    "            for true_ex, pred_ex in zip(true_data, predicted_data):\n",
    "                label_true.append({\n",
    "                    'text': true_ex['text'],\n",
    "                    'entities': [(s, e, l) for s, e, l in true_ex['entities'] if l == label]\n",
    "                })\n",
    "                label_pred.append({\n",
    "                    'text': pred_ex['text'],\n",
    "                    'entities': [(s, e, l) for s, e, l in pred_ex['entities'] if l == label]\n",
    "                })\n",
    "            \n",
    "            per_label_metrics[label] = self.calculate_metrics(label_true, label_pred)\n",
    "        \n",
    "        overall_metrics = self.calculate_metrics(true_data, predicted_data)\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_metrics,\n",
    "            'per_label': per_label_metrics\n",
    "        }\n",
    "    \n",
    "    def create_confusion_matrix(self, true_data: List[Dict], predicted_data: List[Dict]) -> None:\n",
    "        print(\"Creating Confusion Matrix...\")\n",
    "        \n",
    "        all_labels = set()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for true_example, pred_example in zip(true_data, predicted_data):\n",
    "            true_spans = {(s, e): l for s, e, l in true_example['entities']}\n",
    "            pred_spans = {(s, e): l for s, e, l in pred_example['entities']}\n",
    "            \n",
    "            all_spans = set(list(true_spans.keys()) + list(pred_spans.keys()))\n",
    "            \n",
    "            for span in all_spans:\n",
    "                true_label = true_spans.get(span, 'O')  \n",
    "                pred_label = pred_spans.get(span, 'O')\n",
    "                \n",
    "                if true_label != 'O':  \n",
    "                    y_true.append(true_label)\n",
    "                    y_pred.append(pred_label)\n",
    "                    all_labels.add(true_label)\n",
    "                    all_labels.add(pred_label)\n",
    "        \n",
    "        labels = sorted(list(all_labels - {'O'}))\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('NER Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        confusion_file = os.path.join(CONFIG['output_dir'], 'confusion_matrix.png')\n",
    "        plt.savefig(confusion_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Confusion matrix saved to: {confusion_file}\")\n",
    "        \n",
    "        return cm, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERVisualization:\n",
    "    \n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        print(\"Visualization system ready!\")\n",
    "    \n",
    "    def show_predictions(self, text: str, show_in_jupyter=True, save_html=True):\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        if show_in_jupyter:\n",
    "            try:\n",
    "                displacy.render(doc, style='ent', jupyter=True)\n",
    "            except:\n",
    "                self._print_entities(doc)\n",
    "        else:\n",
    "            self._print_entities(doc)\n",
    "        \n",
    "        if save_html:\n",
    "            try:\n",
    "                html = displacy.render(doc, style='ent', page=True)\n",
    "                if html is not None:\n",
    "                    filename = f\"ner_visualization_{hash(text) % 10000}.html\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(html)\n",
    "                    print(f\"Visualization saved to: {filename}\")\n",
    "                else:\n",
    "                    html_content = self._create_simple_html_visualization(doc, text)\n",
    "                    filename = f\"ner_visualization_{hash(text) % 10000}.html\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(html_content)\n",
    "                    print(f\"Simple visualization saved to: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not create HTML visualization: {e}\")\n",
    "                print(\"Displaying text-based visualization instead:\")\n",
    "                self._print_entities(doc)\n",
    "    \n",
    "    def _print_entities(self, doc):\n",
    "        print(f\"Text: {doc.text}\")\n",
    "        print(\"\\nFound entities:\")\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            print(f\"'{ent.text}' -> {ent.label_} (confidence: {ent._.score if hasattr(ent._, 'score') else 'N/A'})\")\n",
    "        \n",
    "        if not doc.ents:\n",
    "            print(\"No entities found.\")\n",
    "    \n",
    "    def _create_simple_html_visualization(self, doc, original_text):\n",
    "        \"\"\"Create a simple HTML visualization when displacy fails\"\"\"\n",
    "        html_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>NER Visualization</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                .entity {{ padding: 2px 4px; margin: 2px; border-radius: 3px; }}\n",
    "                .PER {{ background-color: #ffeaa7; }}\n",
    "                .ORG {{ background-color: #74b9ff; color: white; }}\n",
    "                .LOC {{ background-color: #fd79a8; color: white; }}\n",
    "                .MISC {{ background-color: #a29bfe; color: white; }}\n",
    "                .entity-label {{ font-size: 0.8em; font-weight: bold; }}\n",
    "                .text-container {{ line-height: 1.6; font-size: 18px; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h2>Named Entity Recognition Results</h2>\n",
    "            <div class=\"text-container\">\n",
    "                {highlighted_text}\n",
    "            </div>\n",
    "            <h3>Entities Found:</h3>\n",
    "            <ul>\n",
    "                {entity_list}\n",
    "            </ul>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        highlighted_text = original_text\n",
    "        entity_list = []\n",
    "        \n",
    "        entities = sorted(doc.ents, key=lambda x: x.start_char, reverse=True)\n",
    "        \n",
    "        for ent in entities:\n",
    "            entity_html = f'<span class=\"entity {ent.label_}\">{ent.text} <span class=\"entity-label\">{ent.label_}</span></span>'\n",
    "            highlighted_text = highlighted_text[:ent.start_char] + entity_html + highlighted_text[ent.end_char:]\n",
    "            \n",
    "            entity_list.append(f\"<li><strong>{ent.text}</strong> - {ent.label_}</li>\")\n",
    "        \n",
    "        entity_list_html = \"\\n\".join(entity_list) if entity_list else \"<li>No entities found</li>\"\n",
    "        \n",
    "        return html_template.format(\n",
    "            highlighted_text=highlighted_text,\n",
    "            entity_list=entity_list_html\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a35cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spacy_models(test_data, models_to_compare=['en_core_web_sm', 'en_core_web_md']):\n",
    "    print(f\"Comparing {len(models_to_compare)} spaCy models...\")\n",
    "    \n",
    "    results = {}\n",
    "    loaded_models = {}\n",
    "    \n",
    "    for model_name in models_to_compare:\n",
    "        try:\n",
    "            loaded_models[model_name] = spacy.load(model_name)\n",
    "            print(f\"Loaded {model_name}\")\n",
    "        except OSError:\n",
    "            print(f\"{model_name} not available. Install with: python -m spacy download {model_name}\")\n",
    "            continue\n",
    "    \n",
    "    for model_name, nlp in loaded_models.items():\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "        \n",
    "        predictions = []\n",
    "        for example in test_data:\n",
    "            doc = nlp(example['text'])\n",
    "            entities = []\n",
    "            for ent in doc.ents:\n",
    "                normalized_label = _normalize_spacy_label(ent.label_)\n",
    "                entities.append((ent.start_char, ent.end_char, normalized_label))\n",
    "            predictions.append({'text': example['text'], 'entities': entities})\n",
    "        \n",
    "        evaluator = NEREvaluator()\n",
    "        metrics = evaluator.calculate_metrics(test_data, predictions)\n",
    "        results[model_name] = metrics\n",
    "        \n",
    "        print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"   F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    if len(results) > 1:\n",
    "        _visualize_model_comparison(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb1889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_spacy_label(spacy_label):\n",
    "    mapping = {\n",
    "        'PERSON': 'PER', 'PER': 'PER',\n",
    "        'ORG': 'ORG', 'ORGANIZATION': 'ORG',\n",
    "        'GPE': 'LOC', 'LOC': 'LOC', 'LOCATION': 'LOC', 'FAC': 'LOC',\n",
    "        'NORP': 'MISC', 'PRODUCT': 'MISC', 'EVENT': 'MISC',\n",
    "        'WORK_OF_ART': 'MISC', 'LAW': 'MISC', 'LANGUAGE': 'MISC',\n",
    "        'DATE': 'MISC', 'TIME': 'MISC', 'PERCENT': 'MISC',\n",
    "        'MONEY': 'MISC', 'QUANTITY': 'MISC', 'ORDINAL': 'MISC',\n",
    "        'CARDINAL': 'MISC'\n",
    "    }\n",
    "    return mapping.get(spacy_label, 'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9817370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualize_model_comparison(results):\n",
    "    print(\"Creating model comparison visualization...\")\n",
    "    \n",
    "    models = list(results.keys())\n",
    "    metrics = ['precision', 'recall', 'f1_score']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    f1_scores = [results[model]['f1_score'] for model in models]\n",
    "    bars = axes[0].bar(models, f1_scores, color=['skyblue', 'lightcoral', 'lightgreen'][:len(models)])\n",
    "    axes[0].set_title('F1 Score Comparison')\n",
    "    axes[0].set_ylabel('F1 Score')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model][metric] for model in models]\n",
    "        axes[1].bar(x + i*width, values, width, label=metric.capitalize(), alpha=0.8)\n",
    "    \n",
    "    axes[1].set_title('All Metrics Comparison')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_xlabel('Models')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels(models)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    comparison_file = os.path.join(CONFIG['output_dir'], 'model_comparison.png')\n",
    "    plt.savefig(comparison_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Model comparison saved to: {comparison_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20752de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting the Complete NER Pipeline!\")\n",
    "    \n",
    "    print(\"\\nSTEP 1: Loading Data\")\n",
    "    \n",
    "    data_loader = CoNLLDataLoader()\n",
    "    \n",
    "    try:\n",
    "        train_sentences = data_loader.load_conll_file(CONFIG['data_paths']['train'])\n",
    "        valid_sentences = data_loader.load_conll_file(CONFIG['data_paths']['valid'])\n",
    "        test_sentences = data_loader.load_conll_file(CONFIG['data_paths']['test'])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please make sure your data files are in the correct location!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset Summary:\")\n",
    "    print(f\"   Training sentences: {len(train_sentences)}\")\n",
    "    print(f\"   Validation sentences: {len(valid_sentences)}\")\n",
    "    print(f\"   Test sentences: {len(test_sentences)}\")\n",
    "    \n",
    "    print(\"\\nSTEP 2: Processing Data\")\n",
    "    \n",
    "    processor = NERDataProcessor()\n",
    "    \n",
    "    train_data = processor.process_sentences(train_sentences)\n",
    "    valid_data = processor.process_sentences(valid_sentences)\n",
    "    test_data = processor.process_sentences(test_sentences)\n",
    "    \n",
    "    print(\"Data processing complete!\")\n",
    "    \n",
    "    print(\"\\nSTEP 3: Rule-Based NER\")\n",
    "    \n",
    "    rule_based_ner = RuleBasedNER(spacy.load('en_core_web_sm'))\n",
    "    patterns = rule_based_ner.learn_patterns_from_data(train_data, min_frequency=3)\n",
    "    \n",
    "    test_texts = [example['text'] for example in test_data]\n",
    "    rule_predictions = rule_based_ner.predict(test_texts)\n",
    "    \n",
    "    print(\"Rule-based predictions complete!\")\n",
    "    \n",
    "    print(\"\\nSTEP 4: Model-Based NER\")\n",
    "    \n",
    "    model_based_ner = ModelBasedNER()\n",
    "    \n",
    "    train_subset = train_data[:5000]  \n",
    "    valid_subset = valid_data[:1000]\n",
    "    \n",
    "    training_data = model_based_ner.prepare_training_data(train_subset)\n",
    "    validation_data = model_based_ner.prepare_training_data(valid_subset)\n",
    "    \n",
    "    trained_model = model_based_ner.train_model(\n",
    "        training_data, \n",
    "        validation_data, \n",
    "        iterations=8,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    model_predictions = model_based_ner.predict(test_texts)\n",
    "    \n",
    "    print(\"Model-based predictions complete!\")\n",
    "    \n",
    "    print(\"\\nSTEP 5: Evaluation and Comparison\")\n",
    "    \n",
    "    evaluator = NEREvaluator()\n",
    "    \n",
    "    rule_predictions_norm = evaluator.normalize_predictions(rule_predictions)\n",
    "    model_predictions_norm = evaluator.normalize_predictions(model_predictions)\n",
    "    \n",
    "    rule_metrics = evaluator.detailed_analysis(test_data, rule_predictions_norm)\n",
    "    model_metrics = evaluator.detailed_analysis(test_data, model_predictions_norm)\n",
    "    \n",
    "    print(\"RESULTS COMPARISON:\")\n",
    "    \n",
    "    print(f\"\\nRule-Based NER:\")\n",
    "    print(f\"   Precision: {rule_metrics['overall']['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {rule_metrics['overall']['recall']:.4f}\")\n",
    "    print(f\"   F1 Score:  {rule_metrics['overall']['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nModel-Based NER:\")\n",
    "    print(f\"   Precision: {model_metrics['overall']['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {model_metrics['overall']['recall']:.4f}\")\n",
    "    print(f\"   F1 Score:  {model_metrics['overall']['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Label Performance (Model-Based):\")\n",
    "    for label, metrics in model_metrics['per_label'].items():\n",
    "        print(f\"{label:4s}: P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1_score']:.3f}\")\n",
    "    \n",
    "    print(\"\\nSTEP 6: Enhanced Visualization Demo\")\n",
    "    \n",
    "    visualizer = NERVisualization(trained_model)\n",
    "    \n",
    "    sample_texts = [\n",
    "        \"Apple Inc. is planning to open a new store in New York City.\",\n",
    "        \"Barack Obama was the 44th President of the United States.\",\n",
    "        \"The European Union announced new regulations yesterday.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Sample predictions with HTML visualization:\")\n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        doc = trained_model(text)\n",
    "        print(f\"Text: {text}\")\n",
    "        entities_found = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        print(f\"Entities: {entities_found}\")\n",
    "        \n",
    "        visualizer.show_predictions(text, show_in_jupyter=False, save_html=True)\n",
    "    \n",
    "    print(\"\\nBONUS: Comparing Different spaCy Models\")\n",
    "    comparison_results = compare_spacy_models(test_data[:1000])  \n",
    "    \n",
    "    print(f\"\\nSTEP 7: Saving Results and Final Summary\")\n",
    "    \n",
    "    os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "    model_path = os.path.join(CONFIG['output_dir'], 'trained_ner_model')\n",
    "    trained_model.to_disk(model_path)\n",
    "    \n",
    "    results_path = os.path.join(CONFIG['output_dir'], 'evaluation_results.json')\n",
    "    complete_results = {\n",
    "        'rule_based_metrics': rule_metrics,\n",
    "        'model_based_metrics': model_metrics,\n",
    "        'patterns_learned': len(patterns),\n",
    "        'model_comparison': comparison_results if comparison_results else {},  \n",
    "        'training_summary': {\n",
    "            'training_sentences': len(train_data),  \n",
    "            'validation_sentences': len(valid_data), \n",
    "            'test_sentences': len(test_data),\n",
    "            'early_stopping_triggered': True,\n",
    "            'final_f1_score': model_metrics['overall']['f1_score']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(complete_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    \n",
    "    print(\"\\nFINAL SUMMARY:\")\n",
    "    print(f\" Best F1 Score Achieved: {model_metrics['overall']['f1_score']:.4f}\")\n",
    "    print(f\" Rule-based F1 Score: {rule_metrics['overall']['f1_score']:.4f}\")\n",
    "    print(f\" Model-based F1 Score: {model_metrics['overall']['f1_score']:.4f}\")\n",
    "    print(f\"  Entity Types Recognized: {len(model_metrics['per_label'])} (PER, ORG, LOC, MISC)\")\n",
    "    print(f\" Patterns Learned: {len(patterns)}\")\n",
    "    \n",
    "    print(\"\\nFiles Generated:\")\n",
    "    print(\"â€¢ trained_ner_model/ - Your custom NER model\")\n",
    "    print(\"â€¢ evaluation_results.json - Complete performance metrics\") \n",
    "    print(\"â€¢ confusion_matrix.png - Entity prediction confusion matrix\")\n",
    "    print(\"â€¢ model_comparison.png - spaCy models comparison\")\n",
    "    print(\"â€¢ ner_visualization_*.html - Interactive entity visualizations\")\n",
    "    \n",
    "    print(\"\\nComplete NER Pipeline Finished Successfully!\")\n",
    "    print(\"Your NER system is ready! You can now:\")\n",
    "    print(\"â€¢ Use the trained model for new predictions\")\n",
    "    print(\"â€¢ Compare rule-based vs model-based approaches\") \n",
    "    print(\"â€¢ Visualize entity extraction results with displaCy\")\n",
    "    print(\"â€¢ Analyze performance with confusion matrices\")\n",
    "    print(\"â€¢ Compare different spaCy model performances\")\n",
    "    print(\"â€¢ Fine-tune parameters for even better performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5276709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Complete NER Pipeline!\n",
      "\n",
      "STEP 1: Loading Data\n",
      "Initializing our data loader...\n",
      "Reading Dataset/train.txt...\n",
      "Successfully loaded 14041 sentences from Dataset/train.txt\n",
      "Reading Dataset/valid.txt...\n",
      "Successfully loaded 3250 sentences from Dataset/valid.txt\n",
      "Reading Dataset/test.txt...\n",
      "Successfully loaded 3453 sentences from Dataset/test.txt\n",
      "Dataset Summary:\n",
      "   Training sentences: 14041\n",
      "   Validation sentences: 3250\n",
      "   Test sentences: 3453\n",
      "\n",
      "STEP 2: Processing Data\n",
      "Setting up data processor...\n",
      "Loaded spaCy model: en_core_web_sm\n",
      "Processing 14041 sentences...\n",
      "Data processing complete!\n",
      "Processing 3250 sentences...\n",
      "Data processing complete!\n",
      "Processing 3453 sentences...\n",
      "Data processing complete!\n",
      "Data processing complete!\n",
      "\n",
      "STEP 3: Rule-Based NER\n",
      "Rule-based NER system initialized!\n",
      "Learning patterns from training data...\n",
      "Learned 1986 patterns from training data\n",
      "Rule-based predictions complete!\n",
      "\n",
      "STEP 4: Model-Based NER\n",
      "Initializing model-based NER system...\n",
      "Model-based NER ready with en_core_web_sm\n",
      "Training model for 8 iterations...\n",
      "Training iteration 1/8\n",
      "   Loss: 4180.9751\n",
      "   Validation F1: 0.6840\n",
      "Training iteration 2/8\n",
      "   Loss: 1836.9751\n",
      "   Validation F1: 0.7890\n",
      "Training iteration 3/8\n",
      "   Loss: 1133.4022\n",
      "   Validation F1: 0.7004\n",
      "Training iteration 4/8\n",
      "   Loss: 896.8063\n",
      "   Validation F1: 0.7434\n",
      "Training iteration 5/8\n",
      "   Loss: 808.8304\n",
      "   Validation F1: 0.7217\n",
      "Early stopping triggered!\n",
      "Model training complete!\n",
      "Model-based predictions complete!\n",
      "\n",
      "STEP 5: Evaluation and Comparison\n",
      "NER Evaluator initialized!\n",
      "RESULTS COMPARISON:\n",
      "\n",
      "Rule-Based NER:\n",
      "   Precision: 0.3903\n",
      "   Recall:    0.6392\n",
      "   F1 Score:  0.4847\n",
      "\n",
      "Model-Based NER:\n",
      "   Precision: 0.8040\n",
      "   Recall:    0.8031\n",
      "   F1 Score:  0.8035\n",
      "\n",
      "Per-Label Performance (Model-Based):\n",
      "LOC : P=0.870 R=0.853 F1=0.862\n",
      "ORG : P=0.734 R=0.752 F1=0.743\n",
      "PER : P=0.817 R=0.847 F1=0.832\n",
      "MISC: P=0.785 R=0.704 F1=0.742\n",
      "\n",
      "STEP 6: Enhanced Visualization Demo\n",
      "Visualization system ready!\n",
      "Sample predictions with HTML visualization:\n",
      "\n",
      "Example 1:\n",
      "Text: Apple Inc. is planning to open a new store in New York City.\n",
      "Entities: [('Apple Inc.', 'ORG'), ('New York', 'LOC')]\n",
      "Text: Apple Inc. is planning to open a new store in New York City.\n",
      "\n",
      "Found entities:\n",
      "'Apple Inc.' -> ORG (confidence: N/A)\n",
      "'New York' -> LOC (confidence: N/A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is planning to open a new store in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " City.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple visualization saved to: ner_visualization_5935.html\n",
      "\n",
      "Example 2:\n",
      "Text: Barack Obama was the 44th President of the United States.\n",
      "Entities: [('Barack Obama', 'PER'), ('United States', 'LOC')]\n",
      "Text: Barack Obama was the 44th President of the United States.\n",
      "\n",
      "Found entities:\n",
      "'Barack Obama' -> PER (confidence: N/A)\n",
      "'United States' -> LOC (confidence: N/A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " was the 44th President of the \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple visualization saved to: ner_visualization_5548.html\n",
      "\n",
      "Example 3:\n",
      "Text: The European Union announced new regulations yesterday.\n",
      "Entities: [('European Union', 'ORG')]\n",
      "Text: The European Union announced new regulations yesterday.\n",
      "\n",
      "Found entities:\n",
      "'European Union' -> ORG (confidence: N/A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " announced new regulations yesterday.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple visualization saved to: ner_visualization_6083.html\n",
      "\n",
      "BONUS: Comparing Different spaCy Models\n",
      "Comparing 2 spaCy models...\n",
      "Loaded en_core_web_sm\n",
      "en_core_web_md not available. Install with: python -m spacy download en_core_web_md\n",
      "\n",
      "Testing en_core_web_sm...\n",
      "NER Evaluator initialized!\n",
      "   Precision: 0.3536\n",
      "   Recall:    0.4973\n",
      "   F1 Score:  0.4133\n",
      "\n",
      "STEP 7: Saving Results and Final Summary\n",
      "Model saved to: ner_results\\trained_ner_model\n",
      "Results saved to: ner_results\\evaluation_results.json\n",
      "\n",
      "FINAL SUMMARY:\n",
      " Best F1 Score Achieved: 0.8035\n",
      " Rule-based F1 Score: 0.4847\n",
      " Model-based F1 Score: 0.8035\n",
      "  Entity Types Recognized: 4 (PER, ORG, LOC, MISC)\n",
      " Patterns Learned: 1986\n",
      "\n",
      "Files Generated:\n",
      "â€¢ trained_ner_model/ - Your custom NER model\n",
      "â€¢ evaluation_results.json - Complete performance metrics\n",
      "â€¢ confusion_matrix.png - Entity prediction confusion matrix\n",
      "â€¢ model_comparison.png - spaCy models comparison\n",
      "â€¢ ner_visualization_*.html - Interactive entity visualizations\n",
      "\n",
      "Complete NER Pipeline Finished Successfully!\n",
      "Your NER system is ready! You can now:\n",
      "â€¢ Use the trained model for new predictions\n",
      "â€¢ Compare rule-based vs model-based approaches\n",
      "â€¢ Visualize entity extraction results with displaCy\n",
      "â€¢ Analyze performance with confusion matrices\n",
      "â€¢ Compare different spaCy model performances\n",
      "â€¢ Fine-tune parameters for even better performance\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_tf_env)",
   "language": "python",
   "name": "my_tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
